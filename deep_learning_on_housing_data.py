# -*- coding: utf-8 -*-
"""Deep learning on Housing Data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pk8TpYFW2rqU-Gp-YMS4dw3MhB_Xq4c0
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from google.colab import drive
drive.mount('/content/drive')

#Real world data from kaggle of House sales in king county, USA
df=pd.read_csv("/content/drive/MyDrive/Colab Notebooks/kc_house_data.csv")

"""**Exploratory Data Analysis**"""

df.isnull()

df.isnull().sum()

df.describe()

df.describe().transpose()

plt.figure(figsize=(10,6))
sns.distplot(df['price'])

sns.countplot(df['bedrooms'])

df.corr()['price'].sort_values()

plt.figure(figsize=(10,6))
sns.scatterplot(x='price',y='sqft_living',data=df)

plt.figure(figsize=(10,6))
sns.boxplot(x='bedrooms',y='price',data=df)

df.columns

plt.figure(figsize=(10,6))
sns.scatterplot(x='price',y='long',data=df)

plt.figure(figsize=(10,6))
sns.scatterplot(x='price',y='lat',data=df)

plt.figure(figsize=(10,6))
sns.scatterplot(x='long',y='lat',data=df,hue='price')

df.sort_values('price',ascending=False).head(20)

len(df)

len(df)*0.01  #no of housed in 1%

non_top_1_perc=df.sort_values('price',ascending=False).iloc[216:]

plt.figure(figsize=(10,6))
sns.scatterplot(x='long',y='lat',data=non_top_1_perc,hue='price')

plt.figure(figsize=(12,8))
sns.scatterplot(x='long',y='lat',data=non_top_1_perc,edgecolor=None,alpha=0.2,palette='RdYlGn',hue='price')

sns.boxplot(x='waterfront',y='price',data=df)

df.head()

df=df.drop('id',axis=1)

df['date']=pd.to_datetime(df['date'])

df['date']

def year_extraction(date):
  return date.year

df['year']=df['date'].apply(lambda date:date.year)
df['month']=df['date'].apply(lambda date:date.month)

df.head()

sns.boxplot(x='month',y='price',data=df)

df.groupby('month').mean()['price']

df.groupby('month').mean()['price'].plot()

df.groupby('year').mean()['price'].plot()

df=df.drop('date',axis=1)

df.head()

df['zipcode'].value_counts()

df=df.drop('zipcode',axis=1)

df['yr_renovated'].value_counts()

df['sqft_basement'].value_counts()

plt.figure(figsize=(10,6))
sns.scatterplot(x='yr_renovated',y='price',data=df)

plt.figure(figsize=(10,6))
sns.scatterplot(x='sqft_basement',y='price',data=df)

"""**Data Preprocessing**"""

X=df.drop('price',axis=1).values
y=df['price'].values

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.2, random_state=101)

from sklearn.preprocessing import MinMaxScaler

scaler=MinMaxScaler()

X_train=scaler.fit_transform(X_train) #will scaled all data of X_train between 0 and 1

X_test=scaler.transform(X_test) #will scaled all data of X_test between 0 and 1

"""**Fitting test data and Train data to model**"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

X_train.shape

model=Sequential()
model.add(Dense(19,activation='relu'))
model.add(Dense(19,activation='relu'))
model.add(Dense(19,activation='relu'))
model.add(Dense(19,activation='relu'))
model.add(Dense(1))

model.compile(optimizer='adam',loss='mse')

model.fit(x=X_train,y=y_train,validation_data=(X_test,y_test),batch_size=128,epochs=80)

pd.DataFrame(model.history.history)

losses=  pd.DataFrame(model.history.history)

losses.plot()

from sklearn.metrics import mean_absolute_error,mean_squared_error,explained_variance_score

predictions=model.predict(X_test)

mean_squared_error(y_test,predictions)

np.sqrt(mean_squared_error(y_test,predictions))

mean_absolute_error(y_test,predictions)

df['price'].describe()

5.402966e+05

explained_variance_score(y_test,predictions)

plt.figure(figsize=(12,6))
plt.scatter(y_test,predictions)
plt.plot(y_test,y_test,'r')

single_house=df.drop('price',axis=1).iloc[0]

single_house.values.reshape(-1,19)

single_house=scaler.transform(single_house.values.reshape(-1,19))

model.predict(single_house)

df.head(1)

